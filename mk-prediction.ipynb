{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mk-prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNpkpEwLJahYhB4wwF0Zq5N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satvik-venkatesh/train-synth-audio-seg/blob/main/mk-prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uk1HtQjLdM6y"
      },
      "source": [
        "Please note that the pre-trained models are for non-commercial use only. It is under the [Create Commons Attribution-NonCommercial-ShareAlike-3.0 Unported (CC BY-NC-SA 3.0) license](https://creativecommons.org/licenses/by-nc-sa/3.0/).\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiU4n9kbc-jO"
      },
      "source": [
        "!pip install numba==0.48"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYI9IAnkdaBa"
      },
      "source": [
        "!pip install soundfile==0.10.3.post1\r\n",
        "!sudo apt-get install sox\r\n",
        "!pip install sed_eval\r\n",
        "!pip install librosa==0.7.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYVAJ24gdZa1"
      },
      "source": [
        "!git clone https://github.com/satvik-venkatesh/train-synth-audio-seg.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vzXA43yd2j-"
      },
      "source": [
        "import soundfile as sf\r\n",
        "import numpy as np\r\n",
        "from shutil import copyfile\r\n",
        "import os\r\n",
        "import librosa\r\n",
        "import tensorflow as tf\r\n",
        "import math\r\n",
        "import sed_eval\r\n",
        "import dcase_util\r\n",
        "import json\r\n",
        "import pickle\r\n",
        "import glob\r\n",
        "from subprocess import Popen, PIPE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ9J12x7ZHNu"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJEnwns-nduO"
      },
      "source": [
        "def remove_silence(sound):\n",
        "  \"\"\"\n",
        "  The procedure to remove silence from audio tracks is copied from the Lemaire et al. (2019) github repository.\n",
        "  \"\"\"\n",
        "  temp_file = sound.replace('.wav', '_t.wav').replace('.WAV', '_t.WAV')\n",
        "  command = \"sox \" + sound + \" \" + temp_file + \" silence -l 1 0.1 1% -1 0.1 1%\"\n",
        "  p = Popen(command, stdin=PIPE, stdout=PIPE, stderr=PIPE, shell=True)\n",
        "  output, err = p.communicate()\n",
        "  copyfile(temp_file, sound)\n",
        "  os.remove(temp_file)\n",
        "\n",
        "\n",
        "def get_sound_segments(sound_list, segment_size = 1.0, sampling_rate = 22050.0):\n",
        "  # Load the first sound file.\n",
        "  #remove_silence(sound_list[0])\n",
        "  in_signal, in_sr = sf.read(sound_list[0], dtype='float32')\n",
        "  print(\"in_signal.dtype is {}\".format(in_signal.dtype))\n",
        "\n",
        "  # Resample the audio file.\n",
        "  in_signal_22k = librosa.resample(in_signal, orig_sr=in_sr, target_sr=sampling_rate)\n",
        "  in_signal = np.copy(in_signal_22k)\n",
        "\n",
        "  # Calculate segment size in samples and no of segments\n",
        "  ss = int (sampling_rate * segment_size)\n",
        "  n = int(in_signal.shape[0] / ss)\n",
        "\n",
        "  seg0 = np.reshape(in_signal[0:n * ss], (n, -1))\n",
        "\n",
        "  seg = np.copy(seg0)\n",
        "  # Extract embeddings from remaining files.\n",
        "  for s in sound_list[1:]:\n",
        "    #remove_silence(s)\n",
        "    in_signal, in_sr = sf.read(s)\n",
        "    \n",
        "    # Resample the audio file.\n",
        "    in_signal_22k = librosa.resample(in_signal, orig_sr=in_sr, target_sr=sampling_rate)\n",
        "    in_signal = np.copy(in_signal_22k)\n",
        "    \n",
        "    # Calculate segment size in samples and no of segments\n",
        "    ss = int (sampling_rate * segment_size)\n",
        "    n = int(in_signal.shape[0] / ss)\n",
        "\n",
        "    seg_s = np.reshape(in_signal[0:n * ss], (n, -1))\n",
        "\n",
        "    seg = np.concatenate((seg, seg_s), axis = 0) #em concatenates all the values of embeddings.\n",
        "\n",
        "  return seg \n",
        "\n",
        "\"\"\"\n",
        "A function to extract mel spectrograms.\n",
        "\"\"\"\n",
        "def extract_mel_spec(segments, sr = 22050, hop_length = 220, n_fft = 1024): \n",
        "  # `segments_mel_spec` contains the extracted mel spectrograms.\n",
        "  \n",
        "\n",
        "  # Calculate mel spectrogram of first segment.\n",
        "  s0 = segments[0, :]\n",
        "  mel_spec0 = librosa.feature.melspectrogram(y=s0, sr=sr, hop_length=hop_length, n_fft=n_fft, fmin=64, fmax=8000, n_mels=80)\n",
        "  print('mel_spec0.dtype is {}'.format(mel_spec0.dtype))\n",
        "  # D = librosa.stft(s0, hop_length=hop_length, n_fft=n_fft)\n",
        "  # magnitude, phase = librosa.magphase(D)\n",
        "  # print(\"magnitude.shape is {}\".format(magnitude.shape ))\n",
        "  # print(\"phase.shape is {}\".format(phase.shape ))\n",
        "  # ang_phase = np.angle(phase)\n",
        "  # print(\"ang_phase.shape is {}\".format(ang_phase.shape))\n",
        "  # mel_spec0 = np.concatenate((mel_spec0, ang_phase), axis = 0)\n",
        "\n",
        "  (m, _) = segments.shape\n",
        "  (n, o) = mel_spec0.shape\n",
        "  segments_mel_spec = np.zeros((m, n, o), dtype='float32')\n",
        "  segments_mel_spec[0, :, :] = mel_spec0\n",
        "\n",
        "  for i in range(1, m):\n",
        "    s = segments[i, :]\n",
        "    mel_spec = librosa.feature.melspectrogram(y=s, sr=sr, hop_length=hop_length, n_fft=n_fft, n_mels=80)\n",
        "    # D = librosa.stft(s, hop_length=hop_length, n_fft=n_fft)\n",
        "    # magnitude, phase = librosa.magphase(D)\n",
        "    # ang_phase = np.angle(phase)\n",
        "    # mel_spec = np.concatenate((mel_spec, ang_phase), axis = 0)\n",
        "    segments_mel_spec[i, :, :] = mel_spec\n",
        "\n",
        "  return segments_mel_spec\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "This function was copied from Lemaire et al. 2019. \n",
        "\"\"\"\n",
        "\n",
        "def smooth_output(output, min_speech=1.3, min_music=3.4, max_silence_speech=0.4, max_silence_music=0.6):\n",
        "    duration_frame = 220 / 22050\n",
        "    n_frame = output.shape[1]\n",
        "\n",
        "    start_music = -1000\n",
        "    start_speech = -1000\n",
        "\n",
        "    for i in range(n_frame):\n",
        "        if output[0, i] == 1:\n",
        "            if i - start_speech > 1:\n",
        "                if (i - start_speech) * duration_frame <= max_silence_speech:\n",
        "                    output[0, start_speech:i] = 1\n",
        "\n",
        "            start_speech = i\n",
        "\n",
        "        if output[1, i] == 1:\n",
        "            if i - start_music > 1:\n",
        "                if (i - start_music) * duration_frame <= max_silence_music:\n",
        "                    output[1, start_music:i] = 1\n",
        "\n",
        "            start_music = i\n",
        "\n",
        "    start_music = -1000\n",
        "    start_speech = -1000\n",
        "\n",
        "    for i in range(n_frame):\n",
        "        if i != n_frame - 1:\n",
        "            if output[0, i] == 0:\n",
        "                if i - start_speech > 1:\n",
        "                    if (i - start_speech) * duration_frame <= min_speech:\n",
        "                        output[0, start_speech:i] = 0\n",
        "\n",
        "                start_speech = i\n",
        "\n",
        "            if output[1, i] == 0:\n",
        "                if i - start_music > 1:\n",
        "                    if (i - start_music) * duration_frame <= min_music:\n",
        "                        output[1, start_music:i] = 0\n",
        "\n",
        "                start_music = i\n",
        "        else:\n",
        "            if i - start_speech > 1:\n",
        "                if (i - start_speech) * duration_frame <= min_speech:\n",
        "                    output[0, start_speech:i + 1] = 0\n",
        "\n",
        "            if i - start_music > 1:\n",
        "                if (i - start_music) * duration_frame <= min_music:\n",
        "                    output[1, start_music:i + 1] = 0\n",
        "\n",
        "    return output\n",
        "\n",
        "\"\"\"\n",
        "This function converts the predictions made by the neural network into a format that is understood by sed_eval.\n",
        "\"\"\"\n",
        "\n",
        "def preds_to_se(p, audio_clip_length = 8.0):\n",
        "  start_speech = -100\n",
        "  start_music = -100\n",
        "  stop_speech = -100\n",
        "  stop_music = -100\n",
        "\n",
        "  audio_events = []\n",
        "\n",
        "  n_frames = p.shape[0]\n",
        "\n",
        "  if p[0, 0] == 1:\n",
        "    start_speech = 0\n",
        "  \n",
        "  if p[0, 1] == 1:\n",
        "    start_music = 0\n",
        "\n",
        "  for i in range(n_frames - 1):\n",
        "    if p[i, 0] == 0 and p[i + 1, 0] == 1:\n",
        "      start_speech = i + 1\n",
        "\n",
        "    elif p[i, 0] == 1 and p[i + 1, 0] == 0:\n",
        "      stop_speech = i\n",
        "      start_time = frames_to_time(start_speech)\n",
        "      stop_time = frames_to_time(stop_speech)\n",
        "      audio_events.append((start_time, stop_time, \"speech\"))\n",
        "      start_speech = -100\n",
        "      stop_speech = -100\n",
        "\n",
        "    if p[i, 1] == 0 and p[i + 1, 1] == 1:\n",
        "      start_music = i + 1\n",
        "    elif p[i, 1] == 1 and p[i + 1, 1] == 0:\n",
        "      stop_music = i\n",
        "      start_time = frames_to_time(start_music)\n",
        "      stop_time = frames_to_time(stop_music)      \n",
        "      audio_events.append((start_time, stop_time, \"music\"))\n",
        "      start_music = -100\n",
        "      stop_music = -100\n",
        "\n",
        "  if start_speech != -100:\n",
        "    start_time = frames_to_time(start_speech)\n",
        "    stop_time = audio_clip_length\n",
        "    audio_events.append((start_time, stop_time, \"speech\"))\n",
        "    start_speech = -100\n",
        "    stop_speech = -100\n",
        "\n",
        "  if start_music != -100:\n",
        "    start_time = frames_to_time(start_music)\n",
        "    stop_time = audio_clip_length\n",
        "    audio_events.append((start_time, stop_time, \"music\"))\n",
        "    start_music = -100\n",
        "    stop_music = -100\n",
        "\n",
        "  audio_events.sort(key = lambda x: x[0]) \n",
        "  return audio_events\n",
        "\n",
        "def frames_to_time(f, sr = 22050.0, hop_size = 220):\n",
        "  return f * hop_size / sr\n",
        "\n",
        "def get_log_melspectrogram(audio, sr = 22050, hop_length = 220, n_fft = 1024, n_mels = 80, fmin = 64, fmax = 8000):\n",
        "    \"\"\"Return the log-scaled Mel bands of an audio signal.\"\"\"\n",
        "    bands = librosa.feature.melspectrogram(\n",
        "        y=audio, sr=sr, hop_length=hop_length, n_fft=n_fft, n_mels=n_mels, fmin=fmin, fmax=fmax, dtype=np.float32)\n",
        "    return librosa.core.power_to_db(bands, amin=1e-7)\n",
        "\n",
        "def mk_preds_ens(audio_path, hop_size = 6.0, discard = 1.0, win_length = 8.0, sampling_rate = 22050):\n",
        "  in_signal, in_sr = sf.read(audio_path)\n",
        "\n",
        "  # Convert to mono if needed.\n",
        "  if (in_signal.ndim > 1):\n",
        "    in_signal_mono = librosa.to_mono(in_signal.T)\n",
        "    in_signal = np.copy(in_signal_mono)\n",
        "\n",
        "  # Resample the audio file.\n",
        "  in_signal_22k = librosa.resample(in_signal, orig_sr=in_sr, target_sr=sampling_rate)\n",
        "  in_signal = np.copy(in_signal_22k)\n",
        "\n",
        "  # Pad the input signal if it is shorter than 8 s.\n",
        "  if in_signal.shape[0] < int(8.0 * sampling_rate):\n",
        "  \tpad_signal = np.zeros((int(8.0 * sampling_rate)))\n",
        "  \tpad_signal[:in_signal.shape[0]] = in_signal\n",
        "  \tin_signal = np.copy(pad_signal)\n",
        "\n",
        "\n",
        "  audio_clip_length_samples = in_signal.shape[0]\n",
        "  # print('audio_clip_length_samples is {}'.format(audio_clip_length_samples))\n",
        "\n",
        "  #hop_size_samples = int(hop_size * sampling_rate)\n",
        "  hop_size_samples = 220 * 602 - 1\n",
        "\n",
        "  #win_length_samples = int(win_length * sampling_rate)\n",
        "  win_length_samples = 220 * 802 - 1\n",
        "\n",
        "  n_preds = int(math.ceil((audio_clip_length_samples - win_length_samples) / hop_size_samples)) + 1\n",
        "\n",
        "  #print('n_preds is {}'.format(n_preds))\n",
        "\n",
        "  in_signal_pad = np.zeros((n_preds * hop_size_samples + 200 * 220))\n",
        "\n",
        "  #print('in_signal_pad.shape is {}'.format(in_signal_pad.shape))\n",
        "\n",
        "  in_signal_pad[0:audio_clip_length_samples] = in_signal\n",
        "\n",
        "  preds = np.zeros((n_preds, 802, 2))\n",
        "  mss_in = np.zeros((n_preds, 802, 80))\n",
        "\n",
        "  for i in range(n_preds):\n",
        "    seg = in_signal_pad[i * hop_size_samples:(i * hop_size_samples) + win_length_samples]\n",
        "    #print('seg.shape is {}'.format(seg.shape))\n",
        "    seg = librosa.util.normalize(seg)\n",
        "\n",
        "    mss = get_log_melspectrogram(seg)\n",
        "    M = mss.T\n",
        "    mss_in[i, :, :] = M\n",
        "\n",
        "  yhats = [model.predict(mss_in) for model in models]\n",
        "  yhats = np.array(yhats)\n",
        "  # sum across ensembles\n",
        "  summed = np.mean(yhats, axis=0)\n",
        "\n",
        "  preds = (summed >= (0.5, 0.5)).astype(np.float)\n",
        "  #p = cat_to_multi(p)\n",
        "\n",
        "  # preds[i, :, :] = p[0]\n",
        "\n",
        "  #discard_frames = 100\n",
        "  #oa_preds = np.zeros((, 128)) # overall predictions\n",
        "\n",
        "  preds_mid = np.copy(preds[1:-1, 100:702, :])\n",
        "\n",
        "  #print(\"preds_mid.shape is {}\".format(preds_mid.shape))\n",
        "\n",
        "  preds_mid_2 = preds_mid.reshape(-1, 2)\n",
        "\n",
        "  if preds.shape[0] > 1:\n",
        "    oa_preds = preds[0, 0:702, :] # oa stands for overall predictions\n",
        "\n",
        "  else:\n",
        "    oa_preds = preds[0, 0:802, :] # oa stands for overall predictions\n",
        "\n",
        "  oa_preds = np.concatenate((oa_preds, preds_mid_2), axis = 0)\n",
        "\n",
        "  if preds.shape[0] > 1:\n",
        "    oa_preds = np.concatenate((oa_preds, preds[-1, 100:, :]), axis = 0)\n",
        "\n",
        "  return oa_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSqj4wTUeKYH"
      },
      "source": [
        "# Custom Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xvxu1jGeeM1g"
      },
      "source": [
        "\"\"\"\r\n",
        "Custom Metrics\r\n",
        "\"\"\"\r\n",
        "class SpeechF1(tf.keras.metrics.Metric):\r\n",
        "\r\n",
        "  def __init__(self, name='speech_f1', **kwargs):\r\n",
        "    super(SpeechF1, self).__init__(name=name, **kwargs)\r\n",
        "    self.tp = self.add_weight(name='true_positive', initializer='zeros')\r\n",
        "    self.fp = self.add_weight(name='false_positive', initializer='zeros')\r\n",
        "    self.tn = self.add_weight(name='true_negative', initializer='zeros')\r\n",
        "    self.fn = self.add_weight(name='false_negative', initializer='zeros')\r\n",
        "\r\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\r\n",
        "\r\n",
        "    threshold = tf.constant([0.5])\r\n",
        "\r\n",
        "    binary_true = y_true[:, :, 0]\r\n",
        "    binary_pred = y_pred[:, :, 0]\r\n",
        "\r\n",
        "    binary_true = tf.greater_equal(binary_true, threshold)\r\n",
        "    binary_pred = tf.greater_equal(binary_pred, threshold)\r\n",
        "\r\n",
        "    tp = tf.cast(tf.logical_and(tf.equal(binary_true, True), tf.equal(binary_pred, True)), dtype = np.float32)\r\n",
        "    fp = tf.cast(tf.logical_and(tf.equal(binary_true, False), tf.equal(binary_pred, True)), dtype = np.float32)\r\n",
        "    tn = tf.cast(tf.logical_and(tf.equal(binary_true, False), tf.equal(binary_pred, False)), dtype = np.float32)\r\n",
        "    fn = tf.cast(tf.logical_and(tf.equal(binary_true, True), tf.equal(binary_pred, False)), dtype = np.float32)\r\n",
        "\r\n",
        "    self.tp.assign_add(tf.reduce_sum(tp, axis = None))\r\n",
        "    self.fp.assign_add(tf.reduce_sum(fp, axis = None))\r\n",
        "    self.tn.assign_add(tf.reduce_sum(tn, axis = None))\r\n",
        "    self.fn.assign_add(tf.reduce_sum(fn, axis = None))\r\n",
        "\r\n",
        "  def result(self):\r\n",
        "    binary_f1 = self.tp / (self.tp +  0.5 * (self.fp + self.fn))\r\n",
        "    return binary_f1\r\n",
        "\r\n",
        "  def reset_states(self):\r\n",
        "    self.tp.assign(0)\r\n",
        "    self.fp.assign(0)\r\n",
        "    self.tn.assign(0)\r\n",
        "    self.fn.assign(0)\r\n",
        "\r\n",
        "class MusicF1(tf.keras.metrics.Metric):\r\n",
        "\r\n",
        "  def __init__(self, name='music_f1', **kwargs):\r\n",
        "    super(MusicF1, self).__init__(name=name, **kwargs)\r\n",
        "    self.tp = self.add_weight(name='true_positive', initializer='zeros')\r\n",
        "    self.fp = self.add_weight(name='false_positive', initializer='zeros')\r\n",
        "    self.tn = self.add_weight(name='true_negative', initializer='zeros')\r\n",
        "    self.fn = self.add_weight(name='false_negative', initializer='zeros')\r\n",
        "\r\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\r\n",
        "\r\n",
        "    threshold = tf.constant([0.5])\r\n",
        "\r\n",
        "    binary_true = y_true[:, :, 1]\r\n",
        "    binary_pred = y_pred[:, :, 1]\r\n",
        "\r\n",
        "    binary_true = tf.greater_equal(binary_true, threshold)\r\n",
        "    binary_pred = tf.greater_equal(binary_pred, threshold)\r\n",
        "\r\n",
        "    tp = tf.cast(tf.logical_and(tf.equal(binary_true, True), tf.equal(binary_pred, True)), dtype = np.float32)\r\n",
        "    fp = tf.cast(tf.logical_and(tf.equal(binary_true, False), tf.equal(binary_pred, True)), dtype = np.float32)\r\n",
        "    tn = tf.cast(tf.logical_and(tf.equal(binary_true, False), tf.equal(binary_pred, False)), dtype = np.float32)\r\n",
        "    fn = tf.cast(tf.logical_and(tf.equal(binary_true, True), tf.equal(binary_pred, False)), dtype = np.float32)\r\n",
        "\r\n",
        "    self.tp.assign_add(tf.reduce_sum(tp, axis = None))\r\n",
        "    self.fp.assign_add(tf.reduce_sum(fp, axis = None))\r\n",
        "    self.tn.assign_add(tf.reduce_sum(tn, axis = None))\r\n",
        "    self.fn.assign_add(tf.reduce_sum(fn, axis = None))\r\n",
        "\r\n",
        "  def result(self):\r\n",
        "    binary_f1 = self.tp / (self.tp +  0.5 * (self.fp + self.fn))\r\n",
        "    return binary_f1\r\n",
        "\r\n",
        "  def reset_states(self):\r\n",
        "    self.tp.assign(0)\r\n",
        "    self.fp.assign(0)\r\n",
        "    self.tn.assign(0)\r\n",
        "    self.fn.assign(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W5zSfg4eX76"
      },
      "source": [
        "# Make predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm2jZKzzkPtU"
      },
      "source": [
        "\"\"\"\r\n",
        "'test_audio_dir' is the directory of audio files\r\n",
        "\"\"\"\r\n",
        "test_audio_dir = \"/content/train-synth-audio-seg/Synthetic Radio Examples\"\r\n",
        "test_audio = glob.glob(test_audio_dir + \"/*.wav\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7ekT3_ceZsC"
      },
      "source": [
        "\"\"\"\r\n",
        "This code block performs ensemble prediction for the audio files.\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "expt_seeds = [13, 29, 77, 8, 136]\r\n",
        "\r\n",
        "\"\"\"\r\n",
        "There are 4 possible train_set_types: \"ARE\", \"ARE-RRE\", \"SSE-RRE\", \"SSE\"\r\n",
        "\r\n",
        "ARE: Artificial Radio Examples\r\n",
        "ARE-RRE: Artificial Radio Examples + Real-world Radio Examples\r\n",
        "SSE-RRE: Sound Segment Examples + Real-world Radio Examples\r\n",
        "SSE: Sound Segment Examples\r\n",
        "\r\n",
        "ARE-RRE has best performance.\r\n",
        "\r\n",
        "ARE-RRE > ARE > SSE-RRE > SSE\r\n",
        "\r\n",
        "Please refer to the research paper for more details.\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "train_set_type = \"ARE-RRE\"\r\n",
        "\r\n",
        "models = []\r\n",
        "\r\n",
        "for expt_seed in expt_seeds:\r\n",
        "\r\n",
        "  m = tf.keras.models.load_model(\"/content/train-synth-audio-seg/models/Seed\" + \r\n",
        "                                str(expt_seed) +\"/\"+ train_set_type + \"/model-best.h5\",\r\n",
        "                                    custom_objects={'SpeechF1':SpeechF1(), 'MusicF1':MusicF1()})\r\n",
        "  models.append(m)\r\n",
        "\r\n",
        "\r\n",
        "print(models)\r\n",
        "\r\n",
        "\r\n",
        "for tt in test_audio:\r\n",
        "  ss, _ = sf.read(tt)\r\n",
        "  oop = mk_preds_ens(tt)\r\n",
        "\r\n",
        "  #print(oop.shape)\r\n",
        "  p_smooth = smooth_output(oop.T, min_speech = 0.8, min_music = 3.4, max_silence_speech = 0.8, max_silence_music = 0.8)\r\n",
        "  p_smooth = p_smooth.T\r\n",
        "  see = preds_to_se(p_smooth, audio_clip_length=ss.shape[0]/22050.0)\r\n",
        "  #print(see)\r\n",
        "  n_label = tt.replace(\".wav\", \"-se-prediction.txt\")\r\n",
        "\r\n",
        "  with open(n_label, 'w') as fp:\r\n",
        "    fp.write('\\n'.join('{},{},{}'.format(round(x[0], 5), round(x[1], 5), x[2]) for x in see))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YM4nfodLeh5O"
      },
      "source": [
        "# Code to evaluate predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7DU-1NjelXa"
      },
      "source": [
        "\"\"\"\r\n",
        "Code for second round of using sed_eval starts here.. This simply tests on one of the muspeak datasets\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "eval_path = os.path.join(test_audio_dir, \"Eval\")\r\n",
        "\r\n",
        "try: \r\n",
        "  os.makedirs(eval_path, exist_ok = True) \r\n",
        "  print(\"Directory '%s' created successfully\" %eval_path) \r\n",
        "except OSError as error: \r\n",
        "    print(\"Directory '%s' already exists\")\r\n",
        "\r\n",
        "\r\n",
        "file_list = [\r\n",
        "    {\r\n",
        "    'reference_file': tt.replace(\".wav\", \"-label.txt\"),\r\n",
        "    'estimated_file': tt.replace(\".wav\", \"-se-prediction.txt\")\r\n",
        "    }\r\n",
        "    for tt in test_audio\r\n",
        "]\r\n",
        "\r\n",
        "data = []\r\n",
        "\r\n",
        "# Get used event labels\r\n",
        "all_data = dcase_util.containers.MetaDataContainer()\r\n",
        "for file_pair in file_list:\r\n",
        "    reference_event_list = sed_eval.io.load_event_list(\r\n",
        "        filename=file_pair['reference_file']\r\n",
        "    )\r\n",
        "    estimated_event_list = sed_eval.io.load_event_list(\r\n",
        "        filename=file_pair['estimated_file']\r\n",
        "    )\r\n",
        "\r\n",
        "    data.append({'reference_event_list': reference_event_list,\r\n",
        "                'estimated_event_list': estimated_event_list})\r\n",
        "\r\n",
        "    all_data += reference_event_list\r\n",
        "\r\n",
        "event_labels = all_data.unique_event_labels\r\n",
        "\r\n",
        "# Start evaluating\r\n",
        "\r\n",
        "# Create metrics classes, define parameters\r\n",
        "segment_based_metrics = sed_eval.sound_event.SegmentBasedMetrics(\r\n",
        "    event_label_list=event_labels,\r\n",
        "    time_resolution=0.010\r\n",
        ")\r\n",
        "\r\n",
        "event_based_metrics = sed_eval.sound_event.EventBasedMetrics(\r\n",
        "    event_label_list=event_labels,\r\n",
        "    t_collar=0.500\r\n",
        ")\r\n",
        "\r\n",
        "# Go through files\r\n",
        "for file_pair in data:\r\n",
        "    segment_based_metrics.evaluate(\r\n",
        "        reference_event_list=file_pair['reference_event_list'],\r\n",
        "        estimated_event_list=file_pair['estimated_event_list']\r\n",
        "    )\r\n",
        "\r\n",
        "    event_based_metrics.evaluate(\r\n",
        "        reference_event_list=file_pair['reference_event_list'],\r\n",
        "        estimated_event_list=file_pair['estimated_event_list']\r\n",
        "    )\r\n",
        "\r\n",
        "# Get only certain metrics\r\n",
        "overall_segment_based_metrics = segment_based_metrics.results_overall_metrics()\r\n",
        "print(\"Accuracy:\", overall_segment_based_metrics['accuracy']['accuracy'])\r\n",
        "\r\n",
        "# Or print all metrics as reports\r\n",
        "\r\n",
        "model_basename = train_set_type\r\n",
        "seg_eval_basename = \"seg eval \" + model_basename.replace(\".h5\", \"\") + \".txt\"\r\n",
        "ev_eval_basename = \"event eval \" + model_basename.replace(\".h5\", \"\") + \".txt\"\r\n",
        "with open(os.path.join(eval_path, seg_eval_basename), mode='w') as fp:\r\n",
        "  fp.write(str(segment_based_metrics))\r\n",
        "\r\n",
        "with open(eval_path + \"/seg eval \" + model_basename.replace(\".h5\", \"\") + \".pickle\", 'wb') as f:\r\n",
        "  pickle.dump(segment_based_metrics, f, pickle.HIGHEST_PROTOCOL)\r\n",
        "\r\n",
        "with open(os.path.join(eval_path, ev_eval_basename), mode = 'w') as fp:\r\n",
        "  fp.write(str(event_based_metrics))\r\n",
        "\r\n",
        "with open(eval_path + \"/event eval \" + model_basename.replace(\".h5\", \"\") + \".pickle\", 'wb') as f:\r\n",
        "  pickle.dump(event_based_metrics, f, pickle.HIGHEST_PROTOCOL)   \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQqYvAqGopaY"
      },
      "source": [
        "test_audio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLt9J0tloPB9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}