{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "train-CRNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satvik-venkatesh/train-synth-audio-seg/blob/main/train-CRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEF2proepH9h"
      },
      "source": [
        "!pip install numba==0.48"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iQeu-LxPaeV"
      },
      "source": [
        "!pip install sed_eval\n",
        "!pip install librosa==0.7.2\n",
        "!pip install soundfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaIoY53heKet"
      },
      "source": [
        "!pip install keras-tcn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JJ66pwzrOR4"
      },
      "source": [
        "!pip install -q -U keras-tuner\n",
        "import kerastuner as kt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAr0CO_wC543"
      },
      "source": [
        "import numpy as np\n",
        "import IPython\n",
        "import math\n",
        "import glob\n",
        "import sed_eval\n",
        "import dcase_util\n",
        "import pickle\n",
        "import os\n",
        "import shutil\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras import Input, Model\n",
        "from tcn import TCN\n",
        "from kerastuner import HyperModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2muBoiK7fMKP"
      },
      "source": [
        "\"\"\"\n",
        "Mount Google Drive into Colab.\n",
        "\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpjtJhW3kRPA"
      },
      "source": [
        "\"\"\"\n",
        "Extract artificial data into the 'train data' folder.\n",
        "\"\"\"\n",
        "from zipfile import ZipFile\n",
        "\n",
        "for i in range(0, 8):\n",
        "  zip_name = \"/content/drive/My Drive/Data Synthesis/Train - d_\" + str(i + 1) + \".zip\"\n",
        "  with ZipFile(zip_name, 'r') as zip:\n",
        "    zip.extractall('train data')\n",
        "    print(\"Extracted all sound files into the folder {}\".format(i + 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZT-bvwtnY-9"
      },
      "source": [
        "\"\"\"\n",
        "Extracting Train data (if you have annotated real-world data)\n",
        "\"\"\"\n",
        "from zipfile import ZipFile\n",
        "zip_name = \"/content/drive/My Drive/Data Synthesis/Real-Train.zip\"\n",
        "with ZipFile(zip_name, 'r') as zip:\n",
        "  zip.extractall('train data')\n",
        "  print(\"Extracted all sound files into the folder\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuftbJJSC-oZ"
      },
      "source": [
        "\"\"\"\n",
        "Extracting Real-world Val data\n",
        "\"\"\"\n",
        "from zipfile import ZipFile\n",
        "zip_name = \"/content/drive/My Drive/Data Synthesis/Val - d.zip\"\n",
        "with ZipFile(zip_name, 'r') as zip:\n",
        "  zip.extractall('validation data')\n",
        "  print(\"Extracted all sound files into the folder\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PTvEOVZlteC"
      },
      "source": [
        "def to_seg_by_class(events, n_frames = 802):\n",
        "  labels = np.zeros((n_frames, 2), dtype=np.float32)\n",
        "\n",
        "  for e in events:\n",
        "    t1 = float(e[0])\n",
        "    t1 = int(t1 / 220 * 22050)\n",
        "    t2 = float(e[1])\n",
        "    t2 = int(t2 / 220 * 22050)\n",
        "\n",
        "    if e[2] == 'speech':\n",
        "      labels[t1:t2, 0] = 1\n",
        "    elif e[2] == 'music':\n",
        "      labels[t1:t2, 1] = 1\n",
        "  \n",
        "  return labels "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HqrFP6VxEEJ"
      },
      "source": [
        "\"\"\"\n",
        "Convert the pickle files to npy\n",
        "\"\"\"\n",
        "\n",
        "labels = glob.glob(\"/content/train data/**/mel-id-label-[0-9]*.pickle\", recursive=True)\n",
        "\n",
        "for ll in labels:\n",
        "  with open(ll, 'rb') as f:\n",
        "    n = pickle.load(f)\n",
        "  n2 = to_seg_by_class(n)\n",
        "  np.save(ll.replace(\".pickle\", \".npy\"), n2)\n",
        "\n",
        "\n",
        "labels = glob.glob(\"/content/validation data/**/mel-id-label-[0-9]*.pickle\", recursive=True)\n",
        "\n",
        "for ll in labels:\n",
        "  with open(ll, 'rb') as f:\n",
        "    n = pickle.load(f)\n",
        "  n2 = to_seg_by_class(n)\n",
        "  np.save(ll.replace(\".pickle\", \".npy\"), n2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJ8m-5Kl7JAb"
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "class DataGenerator(tf.compat.v2.keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, list_examples, batch_size=128, dim=(1, ),\n",
        "                 n_classes=2, shuffle=True):\n",
        "        'Initialization'\n",
        "        print(\"Constructor called!!!\")\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.list_examples = list_examples\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        #print(\"The self.list_examples is {}\".format(self.list_examples))\n",
        "        return int(np.floor(len(self.list_examples) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_examples[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "      self.indexes = np.arange(len(self.list_examples))\n",
        "      if self.shuffle == True:\n",
        "          np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        # 'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # # Initialization\n",
        "\n",
        "        X = np.empty([self.batch_size, 802, 80], dtype=np.float32)\n",
        "        y = np.empty([self.batch_size, 802, 2], dtype=np.float32)\n",
        "\n",
        "        # Generate data\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "          # Store sample\n",
        "\n",
        "          xx = np.load(ID[0])\n",
        "          X[i, :, :] = xx\n",
        "\n",
        "          # Store class\n",
        "          yy = np.load(ID[1])\n",
        "                    \n",
        "          y[i, :, :] = yy\n",
        "\n",
        "        return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kh1_gI_vKMO"
      },
      "source": [
        "import re\n",
        "\n",
        "def tryint(s):\n",
        "    try:\n",
        "        return int(s)\n",
        "    except ValueError:\n",
        "        return s\n",
        "    \n",
        "def alphanum_key(s):\n",
        "    \"\"\" Turn a string into a list of string and number chunks.\n",
        "        \"z23a\" -> [\"z\", 23, \"a\"]\n",
        "    \"\"\"\n",
        "    return [ tryint(c) for c in re.split('([0-9]+)', s) ]\n",
        "\n",
        "def sort_nicely(l):\n",
        "    \"\"\" Sort the given list in the way that humans expect.\n",
        "    \"\"\"\n",
        "    l.sort(key=alphanum_key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx3diPkTbs-W"
      },
      "source": [
        "import glob\n",
        "import random\n",
        "\"\"\"\n",
        "Load the individual numpy arrays into partition\n",
        "\"\"\"\n",
        "data = glob.glob(\"/content/train data/**/mel-id-[0-9]*.npy\", recursive=True) # + glob.glob(\"/content/train data/MuSpeak/content/Mel Files/**/mel-id-[0-9]*.npy\", recursive=True) \n",
        "#data = glob.glob(\"/content/train data/MuSpeak/content/Mel Files/**/mel-id-[0-9]*.npy\", recursive=True) \n",
        "sort_nicely(data)\n",
        "\n",
        "labels = glob.glob(\"/content/train data/**/mel-id-label-[0-9]*.npy\", recursive=True) #+ glob.glob(\"/content/train data/MuSpeak/content/Mel Files/**/mel-id-label-[0-9]*.npy\", recursive=True)\n",
        "#labels = glob.glob(\"/content/train data/MuSpeak/content/Mel Files/**/mel-id-label-[0-9]*.npy\", recursive=True)\n",
        "sort_nicely(labels)\n",
        "\n",
        "train_examples = [(data[i], labels[i]) for i in range(len(data))]\n",
        "\n",
        "random.seed(4)\n",
        "random.shuffle(train_examples)\n",
        "\n",
        "partition = {}\n",
        "partition['train'] = train_examples\n",
        "\n",
        "random.shuffle(partition['train'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sianBGv5hkr0"
      },
      "source": [
        "\"\"\"\n",
        "This loads data for the validation set.\n",
        "\"\"\"\n",
        "import glob\n",
        "import random\n",
        "\n",
        "data = glob.glob(\"/content/validation data/**/mel-id-[0-9]*.npy\", recursive=True)\n",
        "sort_nicely(data)\n",
        "\n",
        "labels = glob.glob(\"/content/validation data/**/mel-id-label-[0-9]*.npy\", recursive=True)\n",
        "sort_nicely(labels)\n",
        "\n",
        "validation_examples = [(data[i], labels[i]) for i in range(len(data))]\n",
        "\n",
        "random.seed(4)\n",
        "random.shuffle(validation_examples)\n",
        "\n",
        "partition['validation'] = validation_examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ72mWisst9U"
      },
      "source": [
        "len(partition['train'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZzW248zAFTC"
      },
      "source": [
        "# Parameters\n",
        "params = {'dim': (1, ),\n",
        "          'batch_size': 32,\n",
        "          'n_classes': 2,\n",
        "          'shuffle': True}\n",
        "\n",
        "# Generators\n",
        "training_generator = DataGenerator(partition['train'], **params)\n",
        "validation_generator = DataGenerator(partition['validation'], **params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTNqxZRSr0gP"
      },
      "source": [
        "class SpeechF1(tf.keras.metrics.Metric):\n",
        "\n",
        "  def __init__(self, name='speech_f1', **kwargs):\n",
        "    super(SpeechF1, self).__init__(name=name, **kwargs)\n",
        "    self.tp = self.add_weight(name='true_positive', initializer='zeros')\n",
        "    self.fp = self.add_weight(name='false_positive', initializer='zeros')\n",
        "    self.tn = self.add_weight(name='true_negative', initializer='zeros')\n",
        "    self.fn = self.add_weight(name='false_negative', initializer='zeros')\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "\n",
        "    threshold = tf.constant([0.5])\n",
        "\n",
        "    binary_true = y_true[:, :, 0]\n",
        "    binary_pred = y_pred[:, :, 0]\n",
        "\n",
        "    binary_true = tf.greater_equal(binary_true, threshold)\n",
        "    binary_pred = tf.greater_equal(binary_pred, threshold)\n",
        "\n",
        "    tp = tf.cast(tf.logical_and(tf.equal(binary_true, True), tf.equal(binary_pred, True)), dtype = np.float32)\n",
        "    fp = tf.cast(tf.logical_and(tf.equal(binary_true, False), tf.equal(binary_pred, True)), dtype = np.float32)\n",
        "    tn = tf.cast(tf.logical_and(tf.equal(binary_true, False), tf.equal(binary_pred, False)), dtype = np.float32)\n",
        "    fn = tf.cast(tf.logical_and(tf.equal(binary_true, True), tf.equal(binary_pred, False)), dtype = np.float32)\n",
        "\n",
        "    self.tp.assign_add(tf.reduce_sum(tp, axis = None))\n",
        "    self.fp.assign_add(tf.reduce_sum(fp, axis = None))\n",
        "    self.tn.assign_add(tf.reduce_sum(tn, axis = None))\n",
        "    self.fn.assign_add(tf.reduce_sum(fn, axis = None))\n",
        "\n",
        "  def result(self):\n",
        "    binary_f1 = self.tp / (self.tp +  0.5 * (self.fp + self.fn))\n",
        "    return binary_f1\n",
        "\n",
        "  def reset_states(self):\n",
        "    self.tp.assign(0)\n",
        "    self.fp.assign(0)\n",
        "    self.tn.assign(0)\n",
        "    self.fn.assign(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzT1FpeE1HGC"
      },
      "source": [
        "class MusicF1(tf.keras.metrics.Metric):\n",
        "\n",
        "  def __init__(self, name='music_f1', **kwargs):\n",
        "    super(MusicF1, self).__init__(name=name, **kwargs)\n",
        "    self.tp = self.add_weight(name='true_positive', initializer='zeros')\n",
        "    self.fp = self.add_weight(name='false_positive', initializer='zeros')\n",
        "    self.tn = self.add_weight(name='true_negative', initializer='zeros')\n",
        "    self.fn = self.add_weight(name='false_negative', initializer='zeros')\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "\n",
        "    threshold = tf.constant([0.5])\n",
        "\n",
        "    binary_true = y_true[:, :, 1]\n",
        "    binary_pred = y_pred[:, :, 1]\n",
        "\n",
        "    binary_true = tf.greater_equal(binary_true, threshold)\n",
        "    binary_pred = tf.greater_equal(binary_pred, threshold)\n",
        "\n",
        "    tp = tf.cast(tf.logical_and(tf.equal(binary_true, True), tf.equal(binary_pred, True)), dtype = np.float32)\n",
        "    fp = tf.cast(tf.logical_and(tf.equal(binary_true, False), tf.equal(binary_pred, True)), dtype = np.float32)\n",
        "    tn = tf.cast(tf.logical_and(tf.equal(binary_true, False), tf.equal(binary_pred, False)), dtype = np.float32)\n",
        "    fn = tf.cast(tf.logical_and(tf.equal(binary_true, True), tf.equal(binary_pred, False)), dtype = np.float32)\n",
        "\n",
        "    self.tp.assign_add(tf.reduce_sum(tp, axis = None))\n",
        "    self.fp.assign_add(tf.reduce_sum(fp, axis = None))\n",
        "    self.tn.assign_add(tf.reduce_sum(tn, axis = None))\n",
        "    self.fn.assign_add(tf.reduce_sum(fn, axis = None))\n",
        "\n",
        "  def result(self):\n",
        "    binary_f1 = self.tp / (self.tp +  0.5 * (self.fp + self.fn))\n",
        "    return binary_f1\n",
        "\n",
        "  def reset_states(self):\n",
        "    self.tp.assign(0)\n",
        "    self.fp.assign(0)\n",
        "    self.tn.assign(0)\n",
        "    self.fn.assign(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fncNqYl7wVx"
      },
      "source": [
        "initial_learning_rate = 0.001\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate,\n",
        "    decay_steps=2500,\n",
        "    decay_rate=0.84,\n",
        "    staircase=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww07a2_OIiUs"
      },
      "source": [
        "\n",
        "import os\n",
        "class MyCustomCallback_3(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, model_dir, patience=0):\n",
        "    super(MyCustomCallback_3, self).__init__()\n",
        "    self.patience = patience\n",
        "    # best_weights to store the weights at which the minimum loss occurs.\n",
        "    self.best_weights = None\n",
        "    self.model_best_path = os.path.join(model_dir, 'model-best.h5')\n",
        "    self.model_last_path = os.path.join(model_dir, 'model-last-epoch.h5')\n",
        "    self.custom_params = {\"best_loss\":np.inf, \"last_epoch\":0, \"best_binary_accuracy\":0}\n",
        "    \n",
        "    self.custom_params_path = os.path.join(model_dir, 'custom_params.pickle')\n",
        "    if os.path.isfile(self.custom_params_path):\n",
        "      with open(self.custom_params_path, 'rb') as f:\n",
        "        self.custom_params = pickle.load(f)\n",
        "      best_model = tf.keras.models.load_model(self.model_best_path, custom_objects={ \n",
        "                  'binary_acc':binary_acc, 'TCN':TCN(), 'SpeechF1':SpeechF1(), 'MusicF1':MusicF1()})\n",
        "      self.best_weights = best_model.get_weights()\n",
        "\n",
        "\n",
        "  def on_train_begin(self, logs=None):\n",
        "    # The number of epoch it has waited when loss is no longer minimum.\n",
        "    self.wait = 0\n",
        "    # The epoch the training stops at.\n",
        "    self.stopped_epoch = 0\n",
        "    # Initialize the best F1 as 0.0.\n",
        "    self.is_impatient = False\n",
        "\n",
        "  def on_train_end(self, logs=None):\n",
        "    if not self.is_impatient:\n",
        "      print(\"Restoring model weights from the end of the best epoch.\")\n",
        "      self.model.set_weights(self.best_weights)\n",
        "      # temp_model_path = self.model_path.replace(\".h5\", \"_temp.h5\")\n",
        "      #os.remove(temp_model_path)\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    current_val_loss = logs.get(\"val_loss\")\n",
        "    current_binary_accuracy = logs.get(\"val_binary_accuracy\")\n",
        "    self.model.save(self.model_last_path)\n",
        "    self.custom_params[\"last_epoch\"] = self.custom_params[\"last_epoch\"] + 1\n",
        "\n",
        "    if current_binary_accuracy > self.custom_params['best_binary_accuracy']:\n",
        "      self.custom_params['best_binary_accuracy'] = current_binary_accuracy\n",
        "      self.custom_params['best_loss'] = current_val_loss\n",
        "      self.wait = 0\n",
        "      self.best_weights = self.model.get_weights()\n",
        "      self.model.save(self.model_best_path)\n",
        "\n",
        "    else:\n",
        "        self.wait += 1\n",
        "        if self.wait >= self.patience:\n",
        "            self.stopped_epoch = epoch\n",
        "            self.is_impatient = True\n",
        "            self.model.stop_training = True\n",
        "            print(\"Restoring model weights from the end of the best epoch.\")\n",
        "            self.model.set_weights(self.best_weights)\n",
        "            #os.remove(temp_model_path)\n",
        "    with open(self.custom_params_path, 'wb') as f:\n",
        "      pickle.dump(self.custom_params, f, pickle.HIGHEST_PROTOCOL)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLZuJMy85JmV"
      },
      "source": [
        "\"\"\"\r\n",
        "HP optimised CRNN\r\n",
        "\"\"\"\r\n",
        "mel_input = keras.Input(shape=(802, 80), name=\"mel_input\")\r\n",
        "X = mel_input\r\n",
        "\r\n",
        "X = tf.keras.layers.Reshape((802, 80, 1))(X)\r\n",
        "\r\n",
        "X = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1, padding='same')(X)\r\n",
        "X = tf.keras.layers.Activation('relu')(X)\r\n",
        "X = tf.keras.layers.MaxPool2D(pool_size=(1, 2))(X)\r\n",
        "X = layers.LayerNormalization(axis = [-2, -1])(X)\r\n",
        "\r\n",
        "X = tf.keras.layers.Conv2D(filters=64, kernel_size=11, strides=1, padding='same')(X)\r\n",
        "X = tf.keras.layers.Activation('relu')(X)\r\n",
        "X = tf.keras.layers.MaxPool2D(pool_size=(1, 2))(X)\r\n",
        "X = layers.LayerNormalization(axis = [-2, -1])(X)\r\n",
        "\r\n",
        "\r\n",
        "X = tf.keras.layers.Conv2D(filters=16, kernel_size=11, strides=1, padding='same')(X)\r\n",
        "X = tf.keras.layers.Activation('relu')(X)\r\n",
        "X = tf.keras.layers.MaxPool2D(pool_size=(1, 2))(X)\r\n",
        "X = layers.LayerNormalization(axis = [-2, -1])(X)\r\n",
        "_, _, sx, sy = X.shape\r\n",
        "X = tf.keras.layers.Reshape((-1, int(sx * sy)))(X)\r\n",
        "\r\n",
        "X = layers.Bidirectional(layers.GRU(80, return_sequences = True))(X)\r\n",
        "X = layers.LayerNormalization()(X)\r\n",
        "\r\n",
        "\r\n",
        "X = layers.Bidirectional(layers.GRU(40, return_sequences = True))(X)\r\n",
        "X = layers.LayerNormalization()(X)\r\n",
        "\r\n",
        "pred = layers.Dense(2, name=\"speech_and_music\", activation='sigmoid')(X)\r\n",
        "\r\n",
        "model = keras.Model(inputs = [mel_input], outputs = [pred])\r\n",
        "\r\n",
        "keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)\r\n",
        "\r\n",
        "model.compile(\r\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\r\n",
        "    loss=[tf.keras.losses.BinaryCrossentropy()], metrics=[tf.keras.metrics.BinaryAccuracy(), SpeechF1(), MusicF1()]\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-e6UfJnlIvM"
      },
      "source": [
        "\"\"\"\r\n",
        "Create a directory to train the model.\r\n",
        "\"\"\"\r\n",
        "os.mkdir(\"/content/drive/MyDrive/Models-2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdF9VFqOI8Xv"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "root_dir = \"/content/drive/MyDrive/Models-2\"\n",
        "model_name = 'CRNN-2'\n",
        "model_dir = os.path.join(root_dir, model_name)\n",
        "\n",
        "try: \n",
        "    os.mkdir(model_dir) \n",
        "except OSError as error: \n",
        "    pass  \n",
        "\n",
        "%load_ext tensorboard\n",
        "import datetime, os\n",
        "logdir = os.path.join(root_dir)\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(os.path.join(root_dir, model_name), histogram_freq=1)\n",
        "\n",
        "%tensorboard --logdir \"{logdir}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4fL8Qbf2WpF"
      },
      "source": [
        "initial_epoch = 0\n",
        "p = os.path.join(model_dir, 'custom_params.pickle')\n",
        "if os.path.isfile(p):\n",
        "  print(\"Entered if!!!\")\n",
        "  with open(p, 'rb') as f:\n",
        "    custom_params = pickle.load(f)\n",
        "    last_epoch = custom_params['last_epoch']\n",
        "    initial_epoch = last_epoch\n",
        "  model_path = os.path.join(model_dir, 'model-last-epoch.h5')\n",
        "  print(model_path)\n",
        "  model = tf.keras.models.load_model(model_path, custom_objects={ \n",
        "                  'binary_acc':binary_acc, 'TCN':TCN(), 'SpeechF1':SpeechF1(), 'MusicF1':MusicF1()})\n",
        "  # model.load_weights(model_path)\n",
        "  model.fit(training_generator, validation_data=validation_generator, epochs=300, initial_epoch = initial_epoch, \n",
        "            callbacks=[MyCustomCallback_3(model_dir, patience=20), tensorboard_callback], verbose=2)\n",
        "\n",
        "else:\n",
        "  print(\"Entered else!!!\")\n",
        "\n",
        "  model.fit(training_generator, validation_data=validation_generator, epochs=300,\n",
        "            callbacks=[MyCustomCallback_3(model_dir, patience=20), tensorboard_callback], verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yE-m1Hya3o9F"
      },
      "source": [
        "print(\"Training is complete!!!\")\r\n",
        "import datetime\r\n",
        "now = datetime.datetime.now()\r\n",
        "print (\"Current date and time : \")\r\n",
        "print (now.strftime(\"%Y-%m-%d %H:%M:%S\"))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}